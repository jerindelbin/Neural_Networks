{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30730ee3-04c8-42a7-8569-5d452cff5a5a",
   "metadata": {},
   "source": [
    "**ACTIVATION FUNCTIONS IN DEEP LEARNING LAYERS**\n",
    "\n",
    "**Collection ht**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a525e5c8-88f4-4f3d-8db6-28fd7584fd22",
   "metadata": {},
   "source": [
    "** **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cd3c2a-6686-49a7-8acf-600400df0bc5",
   "metadata": {},
   "source": [
    "**SIGMOID FUNCTION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1952f6ea-8b7b-4b03-949c-730be81d87a6",
   "metadata": {},
   "source": [
    "> Sigmoid Function produces a 'S' shaped Sigmoid Curve. It is used to map the required values to a range between 0 and 1.\n",
    "\n",
    "> Formula: sigmoid(x) = 1/(1+e^-x)\n",
    "\n",
    "> It maps large negative values to values close to 0 and large positive values to values close to 1.\n",
    "\n",
    "> Useful for gradient-based optimization algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f024b29-ad2c-46b0-a24d-0c3dde33bcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74e0cd1c-18fa-41ad-8c09-e56cc258714d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0,\n",
       " 0.7310585786300049,\n",
       " 0.9999999998973812,\n",
       " 4.1399375473943306e-08,\n",
       " 0.9241418199787566)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(100), sigmoid(1), sigmoid(23), sigmoid(-17), sigmoid(2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7f7086-ea53-4e66-b6c7-934ae2df27b5",
   "metadata": {},
   "source": [
    "sigmoid(100): The input is very large and positive, so the output is very close to 1.\n",
    "\n",
    "sigmoid(1): The input is a moderate positive number, so the output is around 0.73.\n",
    "\n",
    "sigmoid(23): The input is very large and positive, so the output is very close to 1.\n",
    "\n",
    "sigmoid(-17): The input is very large and negative, so the output is very close to 0.\n",
    "\n",
    "sigmoid(2.5): The input is a moderate positive number, so the output is around 0.92."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ebc445-a38f-4cae-a931-c7011a83fd5c",
   "metadata": {},
   "source": [
    "** **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2219acff-7e2e-4de3-8a98-2ff0e3aebde0",
   "metadata": {},
   "source": [
    "**Tanh FUNCTION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccbd079-7b95-42e8-991d-878e5b2018cf",
   "metadata": {},
   "source": [
    "> Also produces an 'S' shaped curve. Similar to sigmoid but will have different range. Used to map values between -1 and 1.\n",
    "\n",
    "> Formula: tanh(x) = (e^x - e^-x)/(e^x + e^-x)\n",
    "\n",
    "> It maps large negative values to values close to -1 and large positive values to values close to 1.\n",
    "\n",
    "> tanh function is zero-centered, so its output is symmetrical around the origin. Can help with faster convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d2c421a-f236-457c-9584-e8283ece4a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "  return (math.exp(x) - math.exp(-x)) / (math.exp(x) + math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "618db578-e3ff-4ebc-857b-2051da77c2cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0,\n",
       " 0.7615941559557649,\n",
       " 0.9999999999999966,\n",
       " 1.0,\n",
       " -1.0,\n",
       " -1.0,\n",
       " -0.999999999977259)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tanh(100), tanh(1), tanh(17), tanh(50), tanh(-50), tanh(-25.2), tanh(-12.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5173be0-56c3-4739-a915-ac39704ad866",
   "metadata": {},
   "source": [
    "tanh(100): The input is very large and positive, so the output is very close to 1.\n",
    "\n",
    "tanh(1): The input is a moderate positive number, so the output is around 0.76.\n",
    "\n",
    "tanh(17): The input is large and positive, so the output is very close to 1.\n",
    "\n",
    "tanh(50): The input is very large and positive, so the output is very close to 1.\n",
    "\n",
    "tanh(-50): The input is very large and negative, so the output is very close to -1.\n",
    "\n",
    "tanh(-25.2): The input is very large and negative, so the output is very close to -1.\n",
    "\n",
    "tanh(-12.6): The input is large and negative, so the output is very close to -1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761fe3a0-e602-47ac-800c-874b17aa0b97",
   "metadata": {},
   "source": [
    "** **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2675d746-04ef-410a-83dd-1f3177919dee",
   "metadata": {},
   "source": [
    "**ReLU (RECTIFIED LINEAR UNIT) FUNCTION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30833442-fe4a-4630-86e0-63ea201b1b34",
   "metadata": {},
   "source": [
    "> Used to find the positive part of the values\n",
    "\n",
    "> Formula: ReLU(x)=max(0,x)\n",
    "\n",
    "> The output of the ReLU function ranges from 0 to positive infinity. And outputs zero for all negative input values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8eb431e3-cee0-4120-b362-f9b6c299a808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return max(0,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63a24cee-4e9d-4d49-9064-a5701c2513a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 14)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu(-100), relu(14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10988194-8cb5-4e62-a81f-0d985864ca7f",
   "metadata": {},
   "source": [
    "relu(-100): The input is -100, which is a negative value. So the result is 0.\n",
    "\n",
    "relu(14): The input is 14, which is a positive value. So the result is 14.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ad1f1c-1be7-48aa-aabf-f36c8821503c",
   "metadata": {},
   "source": [
    "** **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd713bf-9b60-4df5-9f9a-d65cc98bd426",
   "metadata": {},
   "source": [
    "**LEAKY ReLU FUNCTION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec8bc3c-e2b8-4004-999e-8988cd8bc7e3",
   "metadata": {},
   "source": [
    "> ReLU has a disadvantage. It outputs zero for all negative inputs, which can lead to \"dying ReLU\" problem where neurons permanently output zero.\n",
    "\n",
    "> Leaky ReLU allows a small, positive gradient for negative inputs, ensuring that the neurons never completely \"die\".\n",
    "\n",
    "> Formula: Leaky ReLU (x) = { x , if x>0 ; a . x, if x<=0 } || a is a small positive slope coefficient =~ 0.01 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c79a97ff-e2b3-471c-af44-2bd08b85d9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(x):\n",
    "    return max(0.1*x,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adbbac19-1259-43ac-96a5-9f4b42e2de02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-10.0, 14)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaky_relu(-100), leaky_relu(14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c6df57-e57e-4de2-82f2-4e5f514b4466",
   "metadata": {},
   "source": [
    "leaky_relu(-100): The input is negative, so Leaky ReLU outputs the input multiplied by a small positive slope coefficient, , in this case -1.0\n",
    "\n",
    "leaky_relu(14): The input is positive, so Leaky ReLU outputs the input directly, which is 14 in this case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
